# Methods

## Gaussian Processes

### Introduction

A Gaussian Process (GP) is a powerful, non-parametric Bayesian approach to modeling distributions over functions. In regression tasks, GPs provide a flexible framework that not only predicts mean function values but also quantifies uncertainty, making them particularly suitable for modeling noisy and complex biological time-series data, such as gene expression profiles.

Formally, a GP is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is fully specified by its mean function $m(\mathbf x)$ and covariance function (kernel) $k(\mathbf x, \mathbf x^\prime)$:

$$
f(\mathbf x) \thicksim GP(m(\mathbf x), k(\mathbf x, \mathbf x^\prime))
$$ {#eq-objective}

For practical applications, and in our case, the mean function is often assumed to be zero ($m(\mathbf x) = 0$) as we can remove trends from our data and then focus on the kernel component which differentiates our models.

### Regression

Given a set of training (for us, time) inputs $\mathbf X = {\mathbf x_1, ..., \mathbf x_n}$ and observations $\mathbf Y = {\mathbf y_1, ..., \mathbf y_n}$; each observation is modelled as $\mathbf y_i = f(\mathbf x_i) + \epsilon_i$ with $\epsilon_i \sim N(0, \sigma_n^2)$ Gaussian noise. The objective is to predict the value $f(\mathbf x_*)$ at new input $\mathbf x_*$.

The joint distribution of the observed values and function at $\mathbf x_*$ is given by:

\begin{align}
\begin{bmatrix}
\mathbf y \\
f_*
\end{bmatrix}
&\sim 
\mathcal N(
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
\mathbf K(\mathbf X, \mathbf X) + \sigma_n^2 I & \mathbf K(\mathbf X, \mathbf x_*) \\
\mathbf K(\mathbf x_*, \mathbf X) & \mathbf K(\mathbf x_*, \mathbf x_*)
\end{bmatrix}
)
\end{align}

Where $\mathbf K(\mathbf X, \mathbf X)$ is the covariance matrix computed over the training inputs and $\mathbf K(\mathbf X, \mathbf x_*)$ is covariance vector between the training inputs and the new input.

The predictive distribution for $f_*$ is then Gaussian with known mean and variance.

### Kernels

The choice of kernel $k(\mathbf x, \mathbf x^\prime)$ is crucial as it determines the behaviour of the model.\
In our case, we will be training models with periodic and aperiodic kernels, then assessing their quality of fits to determine whether or not the underlying gene expression is oscillating or not.

By modelling gene expression time-series generally using the Chemical Master Equation, deriving the Linear Noise Approximation as in @elf_fast_2003, and assuming the deterministic steady-state has been reached, @phillips_identifying_2017 shows that the underlying biological system can be modelled using an Ornstein Uhlenbeck (OU) process.

Thus, to create a pair of periodic and aperiodic kernels, they can take the OU kernel as it is already aperiodic and augment the OU kernel with a cosine kernel to create a quasi-periodic oscillatory process.

\begin{align}
\mathbf K_{OU}(\tau) &= \sigma_{\small OU}\exp(-\alpha \tau) \label{OU_kernel} \\
\mathbf K_{OUosc}(\tau) &= \sigma_{\small OU}\exp(-\alpha \tau)\cos(\beta \tau) \label{OUosc_kernel}
\end{align}

Such that, if the OUosc model has a significantly better fit on a trace despite added model complexity, then it can be reasonably concluded that the trace is oscillatory.

### Inference

#### Closed-Form Posterior and Marginal Likelihood

Gaussian Process regression with a Gaussian likelihood admits an exact posterior, which is itself a GP characterized by an analytically tractable mean and covariance function; @rasmussen2005.

The marginal likelihood (evidence) of the observations under the GP prior can be computed in closed form and differentiated with respect to hyperparameters, enabling Type II Maximum Likelihood (ML) estimation – empirical Bayes – where kernel parameters are set by maximizing the log marginal likelihood.\
Gradient-based optimization (e.g., using L-BFGS or conjugate gradients) is standard practice, as the derivatives of the log marginal likelihood with respect to hyperparameters can be expressed in terms of the inverse and determinant of the kernel matrix.

\newpage

#### Approximate Inference

When the likelihood is non-Gaussian, usually in classification or count data, exact GP inference is intractable, necessitating approximate Bayesian inference.\
These also come into play when we need to numerically integrate the posterior distribution, in our case for calculating the Bayes Factor.

There are many methods here, but we will go over two major ones in GP inference. Variational Inference and Markov Chain Monte Carlo.

##### 1. Markov Chain Monte Carlo (MCMC)

MCMC methods, such as Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampler (NUTS), draw samples from the joint posterior over latent functions and hyperparameters, yielding a full Bayesian treatment with asymptotic exactness; @titsias2011.\
Although more computationally intensive, MCMC does not rely on Gaussian approximations and is widely used when full uncertainty quantification is critical; @titsias2008.

##### 2. Variational Inference

Variational approaches posit a tractable family and optimize a variational lower bound (the evidence lower bound, ELBO) on the log marginal likelihood. Sparse variational methods introduce inducing variables to reduce computational cost, leading to scalable GP models such as the Sparse Variational Gaussian Process; @titsias2009.

------------------------------------------------------------------------

To summarise, while Type II ML provides efficient point estimates of hyperparameters, fully Bayesian approaches use MCMC to marginalize over hyperparameters, capturing posterior uncertainty and avoiding overfitting @titsias2008. The choice between these reflects a trade-off between computational tractability (favoring optimization) and comprehensive uncertainty quantification (favoring sampling).

\newpage

## Bayesian Classification Method

We investigate Bayesian classification methods to classify single-cell gene expression time series as oscillatory or non-oscillatory. This approach builds upon the methodology of @phillips_identifying_2017, but we extend it by using Bayesian inference (via MCMC) for hypothesis testing and uncertainty estimation. The goal is to determine, for each single-cell time course, whether there is statistically significant evidence of an oscillatory pattern as opposed to aperiodic noise. Below, we describe the modeling setup, prior choices, inference procedure, and model comparison techniques we used.

### Overall pipeline

Our method follows mostly @phillips_identifying_2017 until the bootstrapping section. We have calculated the background noise and detrended the input cell traces, then created multiple replicate models for each cell that have been initialised to their maximum likelihood solution (recommended in GPflow docs; @mcmc_gpflow). However, we have only fit the OUosc kernel models (\ref{OUosc_kernel}), as they are all that is needed for the Bayes factor calculation.

Following this, we sample their chains with burn-in, creating posterior samples. Then we pool the traces and assess their convergence using the Gelman-Rubin $\hat R$ and Effective Sample Size (ESS); @gelman1992, @kong_ess.\
Finally, we calculate the Bayes factor using the MCMC samples and classify into oscillatory and non-oscillatory with a cutoff.

### Model Selection and Classification

The next step is to identify the better model and thus, whether or not each trace is oscillatory. In order to do this we calculate the Bayes factor.

The problem setup is as follows. We consider $n \in \mathbb{N}$ competing models and are calculating (relative) plausibility of model $\mathscr{M}_i$ ($i = 1, ..., n$), given the prior probability model and observations $\mathbf{Y}$.\
For this, the posterior model probability $p(\mathscr{M}_i | \mathbf{Y})$, can be expanded using Bayes formula and the law of total conditional probability.

$$
p(\mathscr{M}_i|\mathbf{Y}) = \frac{p(\mathbf{Y}|\mathscr{M}_i)p(\mathscr{M}_i)}{p(\mathbf{Y})} = \frac{p(\mathbf{Y}|\mathscr{M}_i)p(\mathscr{M}_i)}{\sum_{j=1}^m p(\mathbf{Y}|\mathscr{M}_1)p(\mathscr{M}_i)}
$$ {#eq-posterior-prob}

Where the denominator is the sum of the marginal likelihoods times the prior of all $n$ models.\
Taking only two models, as in our scenario, we start to uncover the Bayes factor equation. We can calculate the relative posterior model plausibility of $\mathscr{M}_1$ to $\mathscr{M_2}$ by taking the ratio of the posterior probabilities of both models, also known as the posterior odds; details are in @berger2005.

\begin{align}
\frac{p(\mathscr{M}_1|\mathbf{Y})}{p(\mathscr{M}_2|\mathbf{Y})}
&=
\frac{p(\mathbf{Y}|\mathscr{M}_1)p(\mathscr{M}_1)}{\sum_{j=1}^2 p(\mathbf{Y}|\mathscr{M}_i)p(\mathscr{M}_i)} \times \frac{\sum_{j=1}^2 p(\mathbf{Y}|\mathscr{M}_i)p(\mathscr{M}_i)}{p(\mathbf{Y}|\mathscr{M}_2)p(\mathscr{M}_2)} \\
&= \underbrace{\frac{p(\mathscr{M}_1)}{p(\mathscr{M}_2)}}_{\text{prior odds}} \times \underbrace{\frac{p(\mathbf{Y}|\mathscr{M}_1)}{p(\mathbf{Y}|\mathscr{M}_2)}}_{\text{Bayes factor}}
\end{align}

This shows that our posterior odds are a product of two factors, the prior model odds and the Bayes factor. The Bayes factor is the standard method for hypothesis testing in the Bayesian framework and, as we are using Bayesian inference methods that enable drawing samples from the joint posterior, we can take advantage of this after the fact.

#### Savage-Dickey Ratio

Originally described in @dickey1971 and explained in-depth in @wagenmakers2010; the Savage-Dickey ratio is a method for efficiently calculating the Bayes factor of nested models, requiring only that the support of the prior distribution of the model-difference parameter – $\delta$ – in the larger model includes the null value of that parameter $\delta_0$.\
These requirements allow it to sidestep the requirement for full marginal integration, requiring only posterior samples of the target parameter; it does this by taking the ratio of the posterior to prior density at the null value of the model-difference parameter:

$$
BF_{12} = \frac{p(\mathbf{Y}|\mathscr{M}_1)}{p(\mathbf{Y}|\mathscr{M}_2)} = \frac{p(\delta = \delta_0 | \mathbf{Y}, \mathscr{M}_1)}{p(\delta = \delta_0 | \mathscr{M}_1)}
$$ {#eq-bayes-factor}

This is applicable to our problem, as the non-oscillatory kernel equals the oscillatory kernel at $\beta = 0 \implies cos(\beta \tau) = 1$, and because the prior for that parameter includes $\beta = 0$. So, the Bayes factor is comparing the posterior density of the lengthscale parameter at 0 to the prior density at 0.

We implement this by sampling $\beta$ as part of the MCMC under the oscillatory model In practice, the prior is continuous so the density at exactly 0 is theoretical; we approximate this either by a standard average on the posterior samples, or by calculating a posterior kernel density estimate and evaluating that on our interval.

\newpage

#### Bridge Sampling

Bridge sampling is a flexible Monte Carlo method to approximate marginal likelihoods. Originally introduced in @bennett1976 and formalized by @meng1996, bridge sampling constructs an optimal “bridge” function between two densities – typically the posterior and a convenient proposal – to minimize estimator variance; @gronau2017.\
In its basic form, given the parameter of interest $\theta$'s samples of from two corresponding normalised densities $p_i(\theta) = q_i(\theta) / c_i$ ($i = 1, 2$), the following identity holds:

$$
r \equiv \frac{c_1}{c_2} = \frac{E_2 \left[q_1(\theta) \alpha(\theta)\right]}{E_1 \left[q_2(\theta) \alpha(\theta)\right]}
$$ {#eq-bridge-id}

Where $\alpha(\theta)$ a suitably chosen bridge function. Bennett’s original work framed this in terms of free‐energy differences in physics, then Meng & Wong (1996) extended it to general normalizing‐constant estimation in statistics.\
To then compute the Bayes factor, there are two options:

##### **1. Two-step Approach**

A straightforward strategy is to apply bridge sampling separately to each model to obtain their posterior marginal densities, then form their ratio. This is what is currently implemented in GPCell.

In this, $p_1$ is the fit GP posterior $p(\theta | \mathbf{Y})$ and $p_2$ is a normalised multivariate normal approximation to the posterior (so $c_2$ = 1). The bridge identity (\ref{eq-bridge-id}) then specialises to:

$$
\hat p(\mathbf{Y}) = c_1 = \frac{\frac{1}{n_2}\sum_{i=1}^{n_2}q_1(\omega_i)\alpha(\omega_i)}{\frac{1}{n_1}\sum_{i=1}^{n_1}q_2(\phi_i)\alpha(\phi_i)}
$$

Where $\{\omega_i\}$ are draws from $p_2$ and $\{\phi_i\}$ are draws from the posterior GP.\
A detailed explanation and derivation is available in @meng2002.

##### 2. Direct Ratio Estimation

Alternatively, bridge sampling can directly estimate the ratio $r$, corresponding to the marginal likelihoods of $\mathscr{M}_1$ and $\mathscr{M}_2$ without separate evaluations; @meng2002.\
By treating the posterior distributions under each model as un-normalized densities, one constructs a single bridge estimator for $r$, which is exactly the Bayes factor $B_{12}$.

#### Computational Considerations

While the runtimes of these algorithms are quite fast, there is very little in terms of public implementation for bridge sampling in Python. @gronau2020 provides an R library for bridge sampling, and could be integrated into GPCell using `reticulate` if needed; this is exactly what @else2023 did to use bridge sampling as one of their metrics in a similar setup: *A Deep Learning Method for Comparing Bayesian Hierarchical Models.*

@lao is a single short Jupyter Notebook containing a `pymc` implementation, using functions and methods long since deprecated. As this looks like one of the last general, public implementations, I translated it to modern `pymc`; this can now be used to validate Python methods using other inference backends (like GPflow or GPyTorch).

### **Prior choices**

Our models are formulated in the same way as in @phillips_identifying_2017; however, due to needing being used for the Savage-Dickey ratio and MCMC sampling, our priors need to have a support of $[0, +\infty)$ for $\beta$ as well as being positive and well-behaved in sampling.

@phillips_identifying_2017 used tight, uniform priors for evaluation on the simulated dataset; in order to match these for evaluation against their method,

\newpage

## GPCell

Utilizing the GPflow library, which is based on TensorFlow, GPCell provides a user-friendly interface for researchers, with an `OscillatorDetector` class specifically designed for @phillips_identifying_2017's, and similar, use-cases. It also has unit tests to verify correctness, a strong type system to aid development and upkeep, and a suite of general utility functions that automate the model fitting process, using a multiprocessing pipeline to increase fitting speed.

### Software Architecture

GPCell is structured into clearly defined, modular components. A schematic class diagram of the main components – `OscillatorDetector`, `GaussianProcess`, `GPRConstructor`, and supporting utility modules – is below:

![Class diagram of GPCell](gpcell_UML.png)

`GaussianProcess`**:** An interface for GPflow's Gaussian Process models. It encapsulates the logic for optimising the given model type, predicting on new data, and extracting hyperparameters and values from the kernel. It is instantiated with a `GPRConstructor`, which defines the inference type (i.e: MLE vs MCMC), kernel, priors, and trainable hyperparameters.

This provides a simple interface for users, allows for memory optimisation by only using the posterior data of the fit model after training, and contains individual-process utilities like plotting the fit model and common fit model administration tasks.

`GPRConstructor`**:** This class dynamically instantiates GPflow GP models, which require data, kernels, priors, and trainable hyperparameter flags.

It is separated from `GaussianProcess`, as creating a model and attaching the above is simpler in one step than to construct a `GaussianProcess` with a model type and priors, and then feed in the data dynamically to fit. This is largely due to the structure of GPflow and having to interact with TensorFlow's compute graph.\
These can then be given to multiple GPs being fit in parallel on different CPU processes, making parallelism simpler and allowing `GaussianProcess` objects to be reused on new data with dynamically generated priors.

`OscillatorDetector`**:** Serving as the main user-facing class for oscillation classification, it orchestrates the GP modelling workflow. It takes raw input data, performs preprocessing steps (background noise fitting and detrending), fits oscillatory and non-oscillatory models, and classifies gene expression traces based on statistical criteria such as Bayesian Information Criterion (BIC) or Log-Likelihood Ratios (LLRs).

As well as being a self-contained usable product of the library, it can also be seen as an example of how one can use the library to do analysis.\
A similarly performant, equivalent analysis could be done in a notebook using just the functions in `utils.py` and the users choice of priors and kernels from GPflow. Alternatively, a variety of analyses and extensions could be made to fit and model Gaussian Processes for different use-cases.

`Utils.py`**:** Containing optimised functions such as `fit_processes_joblib` and `get_time_series` (runs Gillespie simulations of the modelled reactions in parallel), the utilities module supports parallel processing via Joblib, facilitating the rapid fitting and generation of large datasets.

### Optimisations and Computational Strategies

The computational complexity associated with GP model fitting – typically $O(n^3)$ – necessitates optimised implementations to handle large bioinformatics datasets efficiently. GPCell employs several optimisation strategies:

**Parallel Processing:** GPCell exploits the parallel processing capabilities provided by `Joblib`. Functions like `fit_processes_joblib` provide general and simple to use utilities that parallelise core functionality. This function significantly reduces compute time by parallelising the fitting of multiple Gaussian Process models across the available CPU cores, yielding substantial performance improvements.

This feature shows some of the big advantages of using Python over MATLAB for this use-case. `Joblib` only has Python dependencies, making it easier to use than the MATLAB library which requires external compilation for the parallelism to work.\
Additionally, `Joblib`'s backend `loky` can pickle or serialise more complex objects, as well as those defined in notebooks, allowing for a more user-friendly interface and efficient encapsulation.\
It also means that, when expanding objects like `GaussianProcess`, users don't have to also add serialisation logic which would be required to store the additional variables required for MCMC inference.

As for performance advantages, `Joblib` supports cached functions across multiple processes, so those used to run Gillespie simulations can be compiled once and then distributed.\
It also provides simple access to `Numpy`'s optimised memmaps, meaning the input time series matrix can be temporarily written to the disk with multiple processes accessing it in parallel, instead of having to give each process its own copy of the experimental/simulated data. As fitting Gaussian Processes requires multiple processes accessing small subsets of the same large file, it is a good fit for memmaps.\
Using memmaps also improves the stability of the parallel processes which is vital for large statistical experiments.

**Just-In-Time Compilation:** For the computationally intensive Gillespie simulations, GPCell utilises `Numba`, a Just-In-Time compiler for Python. This enables the conversion of Python code (exclusively written in base Python and `Numpy`) into optimised machine code at runtime.

This is a perfect use-case for JIT compilation, as the simulation functions themselves are not very complicated computationally, they just need to be ran sequentially for a lot of iterations.

\newpage

### Extensibility and Modularity

GPCell’s design strongly emphasises extensibility, allowing researchers to customise various components to their specific requirements.\
This is enabled by a strong type system integrated throughout the library and the large TensorFlow (Probability) backend.

**Custom Kernels:** GPCell supports the creation and combination of user-defined kernels, facilitating a variety of modelling goals in biology \[find other GPs used in bio/bioinf\]. Users can readily implement new kernels or combine existing ones using arithmetic operations, enhancing model flexibility.

```{python}
# | eval: false

# Kernel types
GPKernel = Union[Type[Kernel], Sequence[Type[Kernel]]]
"""
Type for Gaussian Process kernels.
Can be a single kernel or a list for a composite kernel.
If a list is provided, an operator can be provided or `*` is used.
"""
GPOperator = Callable[[Kernel, Kernel], Kernel]
"""
Type for operators that combine multiple kernels.

Examples for a product `*` operator:
    - `operator.mul` (what is used as default)
    - `gpflow.kernels.Product`
    - `lambda a, b: a * b`
"""
```

Here, `Kernel` is the base class for GPflow's kernels, and is just an extension of TensorFlow's `Module` class which is used to build models in TensorFlow generally.

The code shows that kernels (for use in `GPRConstructor`) can either be a single kernel or a list of kernels, and that the operator that combines them – if multiple are given – is just a callable that takes two kernels and outputs one regardless of operation.\
In this format, $K_{OUosc}$ is defined as:

```{python}
# | eval: false

# Kernels and operator for a GP model
ouosc_kernel = [Matern12, Cosine]
ouosc_op = operator.mul  # which is the default and typically omitted
```

To create a new kernel, one inherits from `Kernel` – or one of its subclasses like `IsotropicStationary`; alternatively, take any class/TensorFlow `Module` and make it compatible with the `Kernel` interface.\
This is the definition of `Matern12` in GPflow as an example.

```{python}
# | eval: false

# Kernel example
class Matern12(IsotropicStationary):
    """
    The kernel equation is
    k(r) = sigma² * exp{-r}
    """

    @check_shapes(
        "r: [batch..., N]",
        "return: [batch..., N]",
    )
    def K_r(self, r: TensorType) -> tf.Tensor:
        return self.variance * tf.exp(-r)
```

It uses GPflow's `IsotropicStationary` kernel base class, which is for stationary kernels that only depend on the Euclidean distance $r = || \mathbf x^\prime - \mathbf x ||_2$, requiring only the implementation of $k(r)$ or $k(r^2)$, $k(r) = k(\mathbf x^\prime, \mathbf x)$.

**Prior Distributions:** Through `GPRConstructor`, researchers can easily define and adjust priors for GP hyperparameters, allowing more precise control over model behaviour and incorporating domain-specific knowledge effectively. This is shown in the typing of `GPPrior` and `GPPriorFactory` in `types.py`:

```{python}
# | eval: false

# Prior types
GPPrior = Mapping[str, Union[Parameter, Numeric]]
"""
Type for Gaussian Process priors.

The keys are the paths to the priors, e.g `.kernel.lengthscales`
or `.kernel.kernels[1].variance`.
Values:
    - `Parameter` used for transformed parameters e.g: Softplus.
    - `Numeric` used for numeric initial values.
"""
GPPriorFactory = Callable[..., GPPrior]
"""
Type for Gaussian Process prior factories.

Used for defining a pattern that will dynamically generate priors
for each GPR model.
"""
```

This gives a clear, type-checked definition of a parameter of `GPRConstructor` (`GPPriorFactory`). That being, a mapping (dictionary) from a string describing its position in the kernel, to either a GPflow `Parameter` – which can be any transformed number for MLE, or a TensorFlow Probability prior distribution for MCMC – or `Numeric`, an unconstrained Python or `Numpy` number.\
This information is all available in the docstring, so is easy to find whilst coding and also gets exported to the auto-generated documentation; there are also examples of each in `backend/priors.py`.

**Inference Techniques:** The modular nature of GPCell allows straightforward integration of different inference techniques, ranging from classical maximum likelihood estimation (MLE) to more complex Bayesian approaches such as variational inference or MCMC.

**MCMC via TensorFlow Probability:** As an example custom/additional inference technique, MCMC functionality has been added to the library just by adding code in two objects – `GPRConstructor` then `GaussianProcess`:

```{python}
# | eval: false

# GPRConstructor
match self.inf:
    case "MCMC":
        likelihood = Gaussian()
        model = GPMC((X, y), kernel, likelihood)
```

This is a simple addition to the `self.inf` check that instantiates a GPflow `GPMC` (MCMC) model instead of a `GPR` (MLE) model, with the rest of the object being the same.

```{python}
# | eval: false

# GaussianProcess
match gp_reg:
    case GPR():
        # Keep the posterior for predictions
        self.fit_gp = gp_reg.posterior()
    case GPMC():
        (
            self.fit_gp,
            self.samples,
            self.parameter_samples,
            self.param_to_name,
            self.name_to_index,
        ) = self._run_mcmc(gp_reg, sampler=self.mcmc_sampler)
```

This directly matches against the model type, improving safety and reducing the number of extra parameters needed.\
Earlier in the `GaussianProcess` fit method, both `GPR` and `GPMC` models are optimised to their MLE positions. This means that `GPR` models are already fit and only the posterior is taken, whereas the `GPMC` is then ran through either a Hamiltonian Monte Carlo (HMC) or No-U-Turn Sampler (NUTS) implemented in TensorFlow Probability, according to `sampler`.