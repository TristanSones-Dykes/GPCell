# Results

## Reproduces Previous Results

A key plot in @phillips_identifying_2017 is the ROC curve plot comparing the GP method to the Lomb-Scargle method. It is reproduced below, with added AUC values for both methods.

![Example simulated traces and model ROC curves, factored by simulated noise level $\sigma^2$ and trace length (time)](ROC_plot.png){width="85%"}

This shows that GPCell fits the same quality models as the MATLAB implementation, on MATLAB generated data. The ROC curves don't have margins on the original plot, adding them makes spotting discrepancies between the models easier.

The added AUC values give a lot of information; despite the $\sigma^2 = 0.5$ ROC for the GP method looking far worse than the others, the AUC shows us it is, quantitatively, not as dissimilar from the others as it looks.

## Performance Improvements

There were two key areas of performance improvement, in fitting models and in simulating data. Some of the improvement was just from switching to MATLAB, but the majority, especially on large datasets, comes from deliberate computational improvements.

### Model Fitting

Using `Joblib`, GPCell is able to distribute the job of fitting a set of replicate models to a trace, as processes ran on separate cores.\
Shown below are the results of fitting the core BIC pipeline (OU and OUosc models) to homogeneous (square/matrix shaped), and non-homogeneous (jagged) datasets. This makes a difference as, through `Joblib`, GPCell is programmed to take advantage of NumPy's `memmap` when possible. It writes the square dataset to a temporary file and provides each process with a read-only reference, allowing concurrent access and reducing memory overhead.

```{r, echo=FALSE, message=FALSE}
#| fig-cap: Time taken to fit processes, on a log scale, split by data source and backend. Base Python at 1000 cells reached 90 minutes before being stopped.

library(tidyverse)

# create dataframe of results
source <- c("homogeneous", "homogeneous", "homogeneous", "homogeneous", "homogeneous", "nonhomog", "nonhomog", "nonhomog")
cells_fit <- c(1000, 1000, 100, 100, 100, 12, 12, 12)
method <- c("base", "joblib", "MATLAB", "base", "joblib", "MATLAB", "base", "joblib")
time <- c(5400, 408, 425.12, 301.39, 51.21, 44.41, 36.09, 12.0)

results <- data.frame(source, cells_fit, method, time)
results$source <- factor(results$source, levels = c("homogeneous", "nonhomog"))
results$method <- factor(results$method, levels = c("base", "joblib", "MATLAB"))
results$time <- as.numeric(results$time)

# plot results
ggplot(results, aes(x = cells_fit, y = time, color = method)) +
  geom_point(aes(shape = source)) +
  geom_line() +
  scale_y_log10() +
  scale_x_log10() +
  labs(
    x = "Number of cells",
    y = "Time (s)",
    color = "Method"
  ) +
  geom_hline(yintercept = 5400, linetype = "dashed", color = "red") +
  annotate("text", x = 15, y = 3000, label = "5400", color = "red", vjust = -1) +
  theme_minimal()
```

This shows the stark difference between fitting models in parallel and in sequence as the number of cells increases.

The datasets for the tests on the larger numbers of cells are all homogeneous. This is because the parallel fitting is more stable when the dataset is able to be made into an optimised `memmap`.\
Note that although it seems like a difficult limit to require the input traces to be homogeneous for the best performance, the performance and stability of the algorithm without it is still a large improvement.

Additionally, during @phillips_identifying_2017's bootstrap step, the synthetic cell generations are homogeneous by definition; this reduces the time to complete such a compute-intensive task massively. If you are only classifying 12 single-cell traces like in their Hes example dataset, that turns into fitting $12 \times 10 \text{ (simulations) } \times 10 \text{ (replicates) = 1200}$ models, this is easily programmed and processed in parallel with GPCell.

### Data Generation

In the original MATLAB implementation, parallel processing is only available with external tooling, and does not take full advantage of vector processing.\
Below is a table comparing the time taken to simulate cells (needed for model evaluation and tuning) at the different levels of optimisation.

```{r, echo=FALSE}
impl_names <- c(
  "Python, parallel and njit",
  "Python, parallel",
  "Python",
  "MATLAB"
)

times <- c(
  "58.64",
  "315.23",
  "3722.16",
  "Approx 1.5 hours"
)

knitr::kable(data.frame(Implementation = impl_names, `Time (s)` = times), caption = "Average data simulation times")
```

Each optimisation decreases the time taken by an order of magnitude, the final optimisation only being possible to due compiling the simulation function (which requires it only be written in NumPy with limited base Python).

These optimisations help in two ways; first by increasing the number of experiments that can be ran and reducing the impact on servers, but also by lowering the computational barrier to use and contribute to the library effectively.

\newpage

## MCMC Support

Show MCMC models being fit through GPCell and other metrics.

Show performance gains also make it to MCMC.

## Bayesian Classifier

Show pymc model fits and plots

Discuss the results of Savage-Dickey vs bridge sampling