{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# set cwd one up\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "path = \"data/hes/Hes1_example.csv\"\n",
    "\n",
    "os.getcwd()\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpcell import plots\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par1 = np.array([300, 1, 0.07, 0.07, 1, 1, 0], dtype=np.float64)\n",
    "par2 = np.array([100, 3, 0.03, 0.03, 1, 1, 18], dtype=np.float64)\n",
    "n_cells = 500  # total replicates used in the simulation\n",
    "t_final_vals = [1500, 600]  # 25/10 hours\n",
    "noise_vals = [np.sqrt(x) for x in [0.1, 0.5]]\n",
    "\n",
    "sim_params = [[noise, t_final_vals[0], n_cells] for noise in noise_vals]\n",
    "sim_params.append([noise_vals[0], t_final_vals[1], n_cells])\n",
    "\n",
    "path = \"data/matlab/\"\n",
    "paths = [\n",
    "    path + f\"noise_{round(noise**2, 2)}_time_{t_final}_rep_{n_cells}.csv\"\n",
    "    for noise, t_final, n_cells in sim_params\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpcell.utils import get_time_series\n",
    "\n",
    "# --- generate data for plot --- #\n",
    "# XY = [\n",
    "#     utils.get_time_series(par1, par2, t_final, noise, n_cells, path=path, mode=\"x\")\n",
    "#     for path, (noise, t_final, n_cells) in zip(paths, sim_params)\n",
    "# ]\n",
    "\n",
    "for path, (noise, t_final, n_cells) in zip(paths, sim_params):\n",
    "    get_time_series(par1, par2, t_final, noise, n_cells, path=path, mode=\"x\")\n",
    "\n",
    "# # save data\n",
    "# for xy, path in zip(XY, paths):\n",
    "#     utils.save_sim(xy[0], xy[1], path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpcell.backend import _simulate_replicate_mod9\n",
    "\n",
    "# --------------------------------#\n",
    "# --- test simulate functions --- #\n",
    "# --------------------------------#\n",
    "\n",
    "# --- test speed --- #\n",
    "# %timeit XY = [utils.get_time_series(par1, par2, t_final, noise, n_cells) for noise, t_final, n_cells in sim_params]\n",
    "\n",
    "# --- test load data --- #\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(paths[0])\n",
    "# print(df.drop(columns=\"Time\").shape[1])\n",
    "\n",
    "# --- test simulate --- #\n",
    "# X, Y = utils.get_time_series(par1, par2, t_final_vals[0], noise_vals[0], n_cells)\n",
    "# print(len(X), len(Y))\n",
    "# print(X[0].shape, Y[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Dev\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.timeseries import LombScargle\n",
    "from gpcell.utils import get_time_series\n",
    "\n",
    "# --- Function parameters --- #\n",
    "filename = paths[0]\n",
    "noise, t_final, n_cells = sim_params[0]\n",
    "joblib = True\n",
    "\n",
    "\n",
    "# ----------------------------#\n",
    "# --- Function definition --- #\n",
    "# ----------------------------#\n",
    "\n",
    "# --- Read the generated data  --- #\n",
    "# The CSV file is assumed to have a \"Time\" column (in hours) and then 2*n_cells columns for the simulated data.\n",
    "x, y_list = get_time_series(\n",
    "    par1, par2, t_final, noise, n_cells, path=filename, mode=\"r\"\n",
    ")\n",
    "total_columns = len(y_list)\n",
    "\n",
    "if total_columns != 2 * n_cells:\n",
    "    raise ValueError(\n",
    "        f\"Expected 2*n_cells = {2 * n_cells} columns, but CSV has {total_columns} columns.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpcell import OscillatorDetector\n",
    "from gpcell.backend.priors import sim_ou_prior, sim_ouosc_prior\n",
    "\n",
    "# --- Fit detector--- #\n",
    "params = {\n",
    "    \"verbose\": True,\n",
    "    \"joblib\": joblib,\n",
    "    \"plots\": [\"BIC\"],\n",
    "    \"set_noise\": noise,\n",
    "    \"detrend\": False,\n",
    "    \"ou_prior_gen\": sim_ou_prior,\n",
    "    \"ouosc_prior_gen\": sim_ouosc_prior,\n",
    "}\n",
    "od = OscillatorDetector.from_file(filename, \"Time\", \"Cell\", \"Cell\", params=params)\n",
    "od.fit(\"BIC\")\n",
    "BICdiffM = od.BIC_diffs  # list or array with length equal to total_columns\n",
    "BICdiffTOT = np.array(BICdiffM)\n",
    "\n",
    "\n",
    "# --- ROC analysis using BIC differences --- #\n",
    "# Split BIC differences into two groups: A (first n_cells) and B (next n_cells)\n",
    "A = BICdiffTOT[:n_cells]\n",
    "B = BICdiffTOT[n_cells : 2 * n_cells]\n",
    "print(A.shape, B.shape)\n",
    "\n",
    "# Define a threshold vector spanning a little below the min to a little above the max.\n",
    "thresholds = np.linspace(BICdiffTOT.min() - 1, BICdiffTOT.max() + 1, 200)\n",
    "FP1 = np.zeros_like(thresholds)\n",
    "TP1 = np.zeros_like(thresholds)\n",
    "for i, th in enumerate(thresholds):\n",
    "    FP1[i] = np.sum(A > th)\n",
    "    TP1[i] = np.sum(B > th)\n",
    "\n",
    "# --- Plot BIC curve --- #\n",
    "od.bic_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrvec = np.exp(np.linspace(-15, 0, 200))\n",
    "n_thr = len(thrvec)\n",
    "n_series = len(y_list)\n",
    "\n",
    "# Preallocate a matrix to store detection results:\n",
    "# rows: time series; columns: thresholds\n",
    "beatmat = np.empty((n_series, n_thr), dtype=int)\n",
    "\n",
    "# Loop once over all time series to compute their periodograms\n",
    "for i, y in enumerate(y_list):\n",
    "    ls = LombScargle(x, y)\n",
    "    frequency, power = ls.autopower(normalization=\"standard\")\n",
    "    # For each threshold, compute false alarm level and determine detection\n",
    "    for j, thr in enumerate(thrvec):\n",
    "        pth = ls.false_alarm_level(thr)\n",
    "        beatmat[i, j] = int(np.any(power > pth))\n",
    "\n",
    "# Compute False Positives (FP) and True Positives (TP) vectorized over thresholds.\n",
    "# Note: following the original code's convention:\n",
    "#   - First half of y_list are considered oscillatory signals.\n",
    "#   - Second half are non-oscillatory.\n",
    "FP2 = (\n",
    "    np.sum(beatmat[:n_cells, :], axis=0) / n_cells\n",
    ")  # fraction detected among first half\n",
    "TP2 = (\n",
    "    np.sum(beatmat[n_cells:, :], axis=0) / n_cells\n",
    ")  # fraction detected among second half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=150)  # Increase DPI for higher definition\n",
    "\n",
    "# Plot lines with thinner width and possibly a z-order to be on top of grid\n",
    "plt.plot(FP1 / n_cells, TP1 / n_cells, label=\"GP\", lw=1, color=\"red\", zorder=2)\n",
    "plt.plot(FP2, TP2, label=\"L-S\", lw=1, color=\"blue\", zorder=2)\n",
    "\n",
    "# Plot the diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], lw=1, linestyle=\"--\", color=\"gray\", zorder=1)\n",
    "\n",
    "# Axis limits\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Axis labels, title, legend\n",
    "plt.xlabel(\"1 - Specificity (false positive rate)\")\n",
    "plt.ylabel(\"Sensitivity (true positive rate)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# If you have a grid, you can draw it behind the lines:\n",
    "# plt.grid(True, zorder=0)\n",
    "\n",
    "plt.tight_layout()  # Better spacing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, (noise, t_final, n_cells) = paths[0], sim_params[0]\n",
    "FP1, TP1, FP2, TP2 = plots.compute_rocs_from_file(path, noise, n_cells, joblib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=150)  # Increase DPI for higher definition\n",
    "\n",
    "# Plot lines with thinner width and possibly a z-order to be on top of grid\n",
    "plt.plot(FP1 / n_cells, TP1 / n_cells, label=\"GP\", lw=1, color=\"red\", zorder=2)\n",
    "plt.plot(FP2, TP2, label=\"L-S\", lw=1, color=\"blue\", zorder=2)\n",
    "\n",
    "# Plot the diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], lw=1, linestyle=\"--\", color=\"gray\", zorder=1)\n",
    "\n",
    "# Axis limits\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Axis labels, title, legend\n",
    "plt.xlabel(\"1 - Specificity (false positive rate)\")\n",
    "plt.ylabel(\"Sensitivity (true positive rate)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# If you have a grid, you can draw it behind the lines:\n",
    "# plt.grid(True, zorder=0)\n",
    "\n",
    "plt.tight_layout()  # Better spacing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROC\n",
    "fp1_vals, fp2_vals, tp1_vals, tp2_vals = [], [], [], []\n",
    "for path, (noise, t_final, n_cells) in zip(paths, sim_params):\n",
    "    fp1, tp1, fp2, tp2 = plots.compute_rocs_from_file(path, noise, n_cells, joblib=True)\n",
    "    fp1_vals.append(fp1)\n",
    "    tp1_vals.append(tp1)\n",
    "    fp2_vals.append(fp2)\n",
    "    tp2_vals.append(tp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# plot ROC\n",
    "x_arr = []\n",
    "dataNormed_arr = []\n",
    "\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    x = df[\"Time\"].values\n",
    "    dataNormed = df.drop(columns=[\"Time\"]).values\n",
    "\n",
    "    x_arr.append(x)\n",
    "    dataNormed_arr.append(dataNormed)\n",
    "\n",
    "plots.plot_rocs_and_timeseries(\n",
    "    x_arr[0],\n",
    "    dataNormed_arr[0],\n",
    "    fp1_vals[0],\n",
    "    tp1_vals[0],\n",
    "    fp2_vals[0],\n",
    "    tp2_vals[0],\n",
    "    x_arr[1],\n",
    "    dataNormed_arr[1],\n",
    "    fp1_vals[1],\n",
    "    tp1_vals[1],\n",
    "    fp2_vals[1],\n",
    "    tp2_vals[1],\n",
    "    x_arr[2],\n",
    "    dataNormed_arr[2],\n",
    "    fp1_vals[2],\n",
    "    tp1_vals[2],\n",
    "    fp2_vals[2],\n",
    "    tp2_vals[2],\n",
    "    n_cells,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiskBackedList created in 0.00202 seconds\n",
      "Before modification: {'b': 2}\n",
      "After modification: {'b': 999}\n",
      "Deleted directory my_objects and its contents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "\n",
    "class DiskBackedProxy:\n",
    "    def __init__(self, index, directory):\n",
    "        # Save metadata without triggering __setattr__\n",
    "        self.__dict__[\"index\"] = index\n",
    "        self.__dict__[\"directory\"] = directory\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        filepath = os.path.join(self.directory, f\"{self.__dict__['index']}.pkl\")\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            self.__dict__[\"_obj\"] = pickle.load(f)\n",
    "\n",
    "    def _save(self):\n",
    "        filepath = os.path.join(self.directory, f\"{self.__dict__['index']}.pkl\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(self.__dict__[\"_obj\"], f)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # Get attribute from the underlying object\n",
    "        attr = getattr(self.__dict__[\"_obj\"], name)\n",
    "        # If the attribute is callable (e.g. a mutating method), wrap it\n",
    "        if callable(attr):\n",
    "\n",
    "            def wrapped(*args, **kwargs):\n",
    "                result = attr(*args, **kwargs)\n",
    "                self._save()\n",
    "                return result\n",
    "\n",
    "            return wrapped\n",
    "        else:\n",
    "            return attr\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        # Delegate attribute assignment to the underlying object\n",
    "        setattr(self.__dict__[\"_obj\"], name, value)\n",
    "        self._save()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        result = self.__dict__[\"_obj\"][key]\n",
    "        return result\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.__dict__[\"_obj\"][key] = value\n",
    "        self._save()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.__dict__[\"_obj\"])\n",
    "\n",
    "\n",
    "class DiskBackedList:\n",
    "    def __init__(self, objects, directory=None):\n",
    "        # Use a default directory if none is provided.\n",
    "        if directory is None:\n",
    "            directory = \"disk_backed_list\"\n",
    "        self.directory = directory\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        self.length = len(objects)\n",
    "        # Save each object as a separate pickle file\n",
    "        for i, obj in enumerate(objects):\n",
    "            filepath = os.path.join(directory, f\"{i}.pkl\")\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(obj, f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            if index < 0:\n",
    "                index += self.length\n",
    "            if index < 0 or index >= self.length:\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            return DiskBackedProxy(index, self.directory)\n",
    "        elif isinstance(index, slice):\n",
    "            indices = range(*index.indices(self.length))\n",
    "            return [self.__getitem__(i) for i in indices]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be an int or slice\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<DiskBackedList of length {self.length} stored in '{self.directory}'>\"\n",
    "\n",
    "    def __del__(self):\n",
    "        # Destructor: delete the directory and all its files when this object is destroyed.\n",
    "        try:\n",
    "            shutil.rmtree(self.directory)\n",
    "            print(f\"Deleted directory {self.directory} and its contents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting directory {self.directory}: {e}\")\n",
    "\n",
    "\n",
    "def disk_backed_list(objects, directory=None):\n",
    "    \"\"\"\n",
    "    Takes an iterable of Python objects, saves each to disk,\n",
    "    and returns an indexable container whose items can be edited in place.\n",
    "    \"\"\"\n",
    "    return DiskBackedList(objects, directory)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # For instance, using a list of dictionaries:\n",
    "    objs = [{\"a\": 1}, {\"b\": 2}, {\"c\": 3}]\n",
    "\n",
    "    start = time.process_time_ns()\n",
    "    dbl = disk_backed_list(objs, directory=\"my_objects\")\n",
    "    end = time.process_time_ns()\n",
    "    print(f\"DiskBackedList created in {(end - start) / 1e9:.5f} seconds\")\n",
    "\n",
    "    # Access the second object (index 1)\n",
    "    obj = dbl[1]\n",
    "    print(\"Before modification:\", obj)\n",
    "\n",
    "    # Modify it in place; if the object is a dict, you can change it directly.\n",
    "    obj[\"b\"] = 999  # This triggers an automatic save.\n",
    "    print(\"After modification:\", dbl[1])\n",
    "\n",
    "    del dbl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
